{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Architecture \u00b6 RancherOS v2 is an immutable Linux distribution built to run Rancher and it's corresponding Kubernetes distributions RKE2 and k3s . It is built using the cOS-toolkit and based on openSUSE. Initial node configurations is done using a cloud-init style approach and all further maintenance is done using Kubernetes operators. Use Cases \u00b6 RancherOS is intended to be ran as the operating system beneath a Rancher Multi-Cluster Management server or as a node in a Kubernetes cluster managed by Rancher. RancherOS also allows you to build stand alone Kubernetes clusters that run an embedded and smaller version of Rancher to manage the local cluster. A key attribute of RancherOS is that it is managed by Rancher and thus Rancher will exist either locally in the cluster or centrally with Rancher Multi-Cluster Manager. OCI Image based \u00b6 RancherOS v2 is an image based distribution with an A/B style update mechanism. One first runs on a read-only image A and to do an upgrade pulls a new read only image B and then reboots the system to run on B. What is unique about RancherOS v2 is that the runtime images come from OCI Images. Not an OCI Image containing special artifacts, but an actual Docker runnable image that is built using standard Docker build processes. RancherOS is built using normal docker build and if you wish to customize the OS image all you need to do is create a new Dockerfile . rancherd \u00b6 RancherOS v2 includes no container runtime, Kubernetes distribution, or Rancher itself. All of these assests are dynamically pulled at runtime. All that is included in RancherOS is rancherd which is responsible for bootstrapping RKE2/k3s and Rancher from an OCI registry. This means an update to containerd, k3s, RKE2, or Rancher does not require an OS upgrade or node reboot. cloud-init \u00b6 RancherOS v2 is initially configured using a simple version of cloud-init . It is not expected that one will need to do a lot of customization to RancherOS as the core OS's sole purpose is to run Rancher and Kubernetes and not serve as a generic Linux distribution. RancherOS Operator \u00b6 RancherOS v2 includes an operator that is responsible for managing OS upgrades and managing a secure device inventory to assist with zero touch provisioning. openSUSE Leap \u00b6 RancherOS v2 is based off of openSUSE Leap. There is no specific dependency on openSUSE beyond that RancherOS assumes the underlying distribution is based on systemd. We choose openSUSE for obvious reasons, but beyond that openSUSE Leap provides a stable layer to build upon that is well tested and has paths to commercial support, if one chooses.","title":"Architecture"},{"location":"#architecture","text":"RancherOS v2 is an immutable Linux distribution built to run Rancher and it's corresponding Kubernetes distributions RKE2 and k3s . It is built using the cOS-toolkit and based on openSUSE. Initial node configurations is done using a cloud-init style approach and all further maintenance is done using Kubernetes operators.","title":"Architecture"},{"location":"#use-cases","text":"RancherOS is intended to be ran as the operating system beneath a Rancher Multi-Cluster Management server or as a node in a Kubernetes cluster managed by Rancher. RancherOS also allows you to build stand alone Kubernetes clusters that run an embedded and smaller version of Rancher to manage the local cluster. A key attribute of RancherOS is that it is managed by Rancher and thus Rancher will exist either locally in the cluster or centrally with Rancher Multi-Cluster Manager.","title":"Use Cases"},{"location":"#oci-image-based","text":"RancherOS v2 is an image based distribution with an A/B style update mechanism. One first runs on a read-only image A and to do an upgrade pulls a new read only image B and then reboots the system to run on B. What is unique about RancherOS v2 is that the runtime images come from OCI Images. Not an OCI Image containing special artifacts, but an actual Docker runnable image that is built using standard Docker build processes. RancherOS is built using normal docker build and if you wish to customize the OS image all you need to do is create a new Dockerfile .","title":"OCI Image based"},{"location":"#rancherd","text":"RancherOS v2 includes no container runtime, Kubernetes distribution, or Rancher itself. All of these assests are dynamically pulled at runtime. All that is included in RancherOS is rancherd which is responsible for bootstrapping RKE2/k3s and Rancher from an OCI registry. This means an update to containerd, k3s, RKE2, or Rancher does not require an OS upgrade or node reboot.","title":"rancherd"},{"location":"#cloud-init","text":"RancherOS v2 is initially configured using a simple version of cloud-init . It is not expected that one will need to do a lot of customization to RancherOS as the core OS's sole purpose is to run Rancher and Kubernetes and not serve as a generic Linux distribution.","title":"cloud-init"},{"location":"#rancheros-operator","text":"RancherOS v2 includes an operator that is responsible for managing OS upgrades and managing a secure device inventory to assist with zero touch provisioning.","title":"RancherOS Operator"},{"location":"#opensuse-leap","text":"RancherOS v2 is based off of openSUSE Leap. There is no specific dependency on openSUSE beyond that RancherOS assumes the underlying distribution is based on systemd. We choose openSUSE for obvious reasons, but beyond that openSUSE Leap provides a stable layer to build upon that is well tested and has paths to commercial support, if one chooses.","title":"openSUSE Leap"},{"location":"ami/","text":"Amazon AMIs \u00b6 AMIs for RancherOS are published under the owner ID 275947076441 in the us-west-1 and us-west-2 regions currently. aws --region = us-west-1 ec2 describe-images --owners 275947076441 aws --region = us-west-2 ec2 describe-images --owners 275947076441","title":"Amazon AMIs"},{"location":"ami/#amazon-amis","text":"AMIs for RancherOS are published under the owner ID 275947076441 in the us-west-1 and us-west-2 regions currently. aws --region = us-west-1 ec2 describe-images --owners 275947076441 aws --region = us-west-2 ec2 describe-images --owners 275947076441","title":"Amazon AMIs"},{"location":"architecture/","text":"Architecture \u00b6 RancherOS v2 is an immutable Linux distribution built to run Rancher and it's corresponding Kubernetes distributions RKE2 and k3s . It is built using the cOS-toolkit and based on openSUSE. Initial node configurations is done using a cloud-init style approach and all further maintenance is done using Kubernetes operators. Use Cases \u00b6 RancherOS is intended to be ran as the operating system beneath a Rancher Multi-Cluster Management server or as a node in a Kubernetes cluster managed by Rancher. RancherOS also allows you to build stand alone Kubernetes clusters that run an embedded and smaller version of Rancher to manage the local cluster. A key attribute of RancherOS is that it is managed by Rancher and thus Rancher will exist either locally in the cluster or centrally with Rancher Multi-Cluster Manager. OCI Image based \u00b6 RancherOS v2 is an image based distribution with an A/B style update mechanism. One first runs on a read-only image A and to do an upgrade pulls a new read only image B and then reboots the system to run on B. What is unique about RancherOS v2 is that the runtime images come from OCI Images. Not an OCI Image containing special artifacts, but an actual Docker runnable image that is built using standard Docker build processes. RancherOS is built using normal docker build and if you wish to customize the OS image all you need to do is create a new Dockerfile . rancherd \u00b6 RancherOS v2 includes no container runtime, Kubernetes distribution, or Rancher itself. All of these assests are dynamically pulled at runtime. All that is included in RancherOS is rancherd which is responsible for bootstrapping RKE2/k3s and Rancher from an OCI registry. This means an update to containerd, k3s, RKE2, or Rancher does not require an OS upgrade or node reboot. cloud-init \u00b6 RancherOS v2 is initially configured using a simple version of cloud-init . It is not expected that one will need to do a lot of customization to RancherOS as the core OS's sole purpose is to run Rancher and Kubernetes and not serve as a generic Linux distribution. RancherOS Operator \u00b6 RancherOS v2 includes an operator that is responsible for managing OS upgrades and managing a secure device inventory to assist with zero touch provisioning. openSUSE Leap \u00b6 RancherOS v2 is based off of openSUSE Leap. There is no specific dependency on openSUSE beyond that RancherOS assumes the underlying distribution is based on systemd. We choose openSUSE for obvious reasons, but beyond that openSUSE Leap provides a stable layer to build upon that is well tested and has paths to commercial support, if one chooses.","title":"Architecture"},{"location":"architecture/#architecture","text":"RancherOS v2 is an immutable Linux distribution built to run Rancher and it's corresponding Kubernetes distributions RKE2 and k3s . It is built using the cOS-toolkit and based on openSUSE. Initial node configurations is done using a cloud-init style approach and all further maintenance is done using Kubernetes operators.","title":"Architecture"},{"location":"architecture/#use-cases","text":"RancherOS is intended to be ran as the operating system beneath a Rancher Multi-Cluster Management server or as a node in a Kubernetes cluster managed by Rancher. RancherOS also allows you to build stand alone Kubernetes clusters that run an embedded and smaller version of Rancher to manage the local cluster. A key attribute of RancherOS is that it is managed by Rancher and thus Rancher will exist either locally in the cluster or centrally with Rancher Multi-Cluster Manager.","title":"Use Cases"},{"location":"architecture/#oci-image-based","text":"RancherOS v2 is an image based distribution with an A/B style update mechanism. One first runs on a read-only image A and to do an upgrade pulls a new read only image B and then reboots the system to run on B. What is unique about RancherOS v2 is that the runtime images come from OCI Images. Not an OCI Image containing special artifacts, but an actual Docker runnable image that is built using standard Docker build processes. RancherOS is built using normal docker build and if you wish to customize the OS image all you need to do is create a new Dockerfile .","title":"OCI Image based"},{"location":"architecture/#rancherd","text":"RancherOS v2 includes no container runtime, Kubernetes distribution, or Rancher itself. All of these assests are dynamically pulled at runtime. All that is included in RancherOS is rancherd which is responsible for bootstrapping RKE2/k3s and Rancher from an OCI registry. This means an update to containerd, k3s, RKE2, or Rancher does not require an OS upgrade or node reboot.","title":"rancherd"},{"location":"architecture/#cloud-init","text":"RancherOS v2 is initially configured using a simple version of cloud-init . It is not expected that one will need to do a lot of customization to RancherOS as the core OS's sole purpose is to run Rancher and Kubernetes and not serve as a generic Linux distribution.","title":"cloud-init"},{"location":"architecture/#rancheros-operator","text":"RancherOS v2 includes an operator that is responsible for managing OS upgrades and managing a secure device inventory to assist with zero touch provisioning.","title":"RancherOS Operator"},{"location":"architecture/#opensuse-leap","text":"RancherOS v2 is based off of openSUSE Leap. There is no specific dependency on openSUSE beyond that RancherOS assumes the underlying distribution is based on systemd. We choose openSUSE for obvious reasons, but beyond that openSUSE Leap provides a stable layer to build upon that is well tested and has paths to commercial support, if one chooses.","title":"openSUSE Leap"},{"location":"clusters/","text":"Understanding Clusters \u00b6 RancherOS bootstraps a node with Kubernetes (k3s/rke2) and Rancher such that all future management of Kubernetes and Rancher can be done from Kubernetes. This is done by running Rancherd once per node on boot. Once the system has been fully bootstrapped it will not run again. Rancherd is ran from cloud-init and it's configuration is embedded in the cloud-config file. Cluster Initialization \u00b6 Creating a cluster always starts with one node initializing the cluster, and all other nodes joining the cluster by pointing to a server node. The node that will initialize a new cluster is the one with role: server and server: \"\" (empty). The new cluster will have a token generated or you can manually assign a unique string. The token for an existing cluster can be determined by running rancherd get-token on a server node. Joining Nodes \u00b6 Nodes can be joined to the cluster as the role server to add more control plane nodes or as the role agent to add more worker nodes. To join a node you must have the Rancher server URL (which is by default running on port 8443 ) and the token. The server and token are assigned to the server and token fields respectively. Node Roles \u00b6 Rancherd will bootstrap a node with one of the following roles server : Joins the cluster as a new control-plane,etcd,worker node agent : Joins the cluster as a worker only node. Server discovery \u00b6 It can be quite cumbersome to automate bringing up a clustered system that requires one bootstrap node. Also there are more considerations around load balancing and replacing nodes in a proper production setup. Rancherd support server discovery based on go-discover . To use server discovery you must set the role , discovery and token fields. The discovery configuration will be used to dynamically determine what is the server URL and if the current node should act as the node to initialize the cluster. Example role : server discovery : params : # Corresponds to go-discover provider name provider : \"mdns\" # All other key/values are parameters corresponding to what # the go-discover provider is expecting service : \"rancher-server\" # If this is a new cluster it will wait until 3 server are # available and they all agree on the same cluster-init node expectedServers : 3 # How long servers are remembered for. It is useful for providers # that are not consistent in their responses, like mdns. serverCacheDuration : 1m","title":"Understanding Clusters"},{"location":"clusters/#understanding-clusters","text":"RancherOS bootstraps a node with Kubernetes (k3s/rke2) and Rancher such that all future management of Kubernetes and Rancher can be done from Kubernetes. This is done by running Rancherd once per node on boot. Once the system has been fully bootstrapped it will not run again. Rancherd is ran from cloud-init and it's configuration is embedded in the cloud-config file.","title":"Understanding Clusters"},{"location":"clusters/#cluster-initialization","text":"Creating a cluster always starts with one node initializing the cluster, and all other nodes joining the cluster by pointing to a server node. The node that will initialize a new cluster is the one with role: server and server: \"\" (empty). The new cluster will have a token generated or you can manually assign a unique string. The token for an existing cluster can be determined by running rancherd get-token on a server node.","title":"Cluster Initialization"},{"location":"clusters/#joining-nodes","text":"Nodes can be joined to the cluster as the role server to add more control plane nodes or as the role agent to add more worker nodes. To join a node you must have the Rancher server URL (which is by default running on port 8443 ) and the token. The server and token are assigned to the server and token fields respectively.","title":"Joining Nodes"},{"location":"clusters/#node-roles","text":"Rancherd will bootstrap a node with one of the following roles server : Joins the cluster as a new control-plane,etcd,worker node agent : Joins the cluster as a worker only node.","title":"Node Roles"},{"location":"clusters/#server-discovery","text":"It can be quite cumbersome to automate bringing up a clustered system that requires one bootstrap node. Also there are more considerations around load balancing and replacing nodes in a proper production setup. Rancherd support server discovery based on go-discover . To use server discovery you must set the role , discovery and token fields. The discovery configuration will be used to dynamically determine what is the server URL and if the current node should act as the node to initialize the cluster. Example role : server discovery : params : # Corresponds to go-discover provider name provider : \"mdns\" # All other key/values are parameters corresponding to what # the go-discover provider is expecting service : \"rancher-server\" # If this is a new cluster it will wait until 3 server are # available and they all agree on the same cluster-init node expectedServers : 3 # How long servers are remembered for. It is useful for providers # that are not consistent in their responses, like mdns. serverCacheDuration : 1m","title":"Server discovery"},{"location":"configuration/","text":"Configuration Reference \u00b6 All configuration should come from RancherOS minimal cloud-init . Below is a reference of supported configuration. It is important that the config always starts with #cloud-config #cloud-config # Add additional users or set the password/ssh keys for root users : - name : \"bar\" passwd : \"foo\" groups : \"users\" ssh_authorized_keys : - faaapploo # Assigns these keys to the first user in users or root if there # is none ssh_authorized_keys : - asdd # Run these commands once the system has fully booted runcmd : - foo # Hostname to assign hostname : \"bar\" # Write arbitrary files write_files : - encoding : b64 content : CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4 path : /foo/bar permissions : \"0644\" owner : \"bar\" # Rancherd configuration rancherd : ######################################################## # The below parameters apply to server role that first # # initializes the cluster # ######################################################## # The Kubernetes version to be installed. This must be a k3s or RKE2 version # v1.21 or newer. k3s and RKE2 versions always have a `k3s` or `rke2` in the # version string. # Valid versions are # k3s: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.k3s.releases[].version' # RKE2: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.rke2.releases[].version' kubernetesVersion : v1.22.2+k3s1 # The Rancher version to be installed or a channel \"latest\" or \"stable\" rancherVersion : v2.6.0 # Values set on the Rancher Helm chart. Refer to # https://github.com/rancher/rancher/blob/release/v2.6/chart/values.yaml # for possible values. rancherValues : # Below are the default values set # Multi-Cluster Management is disabled by default, change to multi-cluster-management=true to enable features : multi-cluster-management=false # The Rancher UI will run on the host port 8443 by default. Set to 0 to disable # and instead use ingress.enabled=true to route traffic through ingress hostPort : 8443 # Accessing ingress is disabled by default. ingress : enabled : false # Don't create a default admin password noDefaultAdmin : true # The negative value means it will up to that many replicas if there are # at least that many nodes available. For example, if you have 2 nodes and # `replicas` is `-3` then 2 replicas will run. Once you add a third node # a then 3 replicas will run replicas : -3 # External TLS is assumed tls : external # Addition SANs (hostnames) to be added to the generated TLS certificate that # served on port 6443. tlsSans : - additionalhostname.example.com # Kubernetes resources that will be created once Rancher is bootstrapped resources : - kind : ConfigMap apiVersion : v1 metadata : name : random data : key : value # Contents of the registries.yaml that will be used by k3s/RKE2. The structure # is documented at https://rancher.com/docs/k3s/latest/en/installation/private-registry/ registries : {} # The default registry used for all Rancher container images. For more information # refer to https://rancher.com/docs/rancher/v2.6/en/admin-settings/config-private-registry/ systemDefaultRegistry : someprefix.example.com:5000 # Advanced: The system agent installer image used for Kubernetes runtimeInstallerImage : ... # Advanced: The system agent installer image used for Rancher rancherInstallerImage : ... # Generic commands to run before bootstrapping the node. preInstructions : - name : something # This image will be extracted to a temporary folder and # set as the current working dir. The command will not run # contained or chrooted, this is only a way to copy assets # to the host. This is parameter is optional image : custom/image:1.1.1 # Environment variables to set env : - FOO=BAR # Program arguments args : - arg1 - arg2 # Command to run command : /bin/dosomething # Save output to /var/lib/rancher/rancherd/plan/plan-output.json saveOutput : false # Generic commands to run after bootstrapping the node. postInstructions : - name : something env : - FOO=BAR args : - arg1 - arg2 command : /bin/dosomething saveOutput : false ########################################### # The below parameters apply to all roles # ########################################### # The URL to Rancher to join a node. If you have disabled the hostPort and configured # TLS then this will be the server you have setup. server : https://myserver.example.com:8443 # A shared secret to join nodes to the cluster token : sometoken # Instead of setting the server parameter above the server value can be dynamically # determined from cloud provider metadata. This is powered by https://github.com/hashicorp/go-discover. # Discovery requires that the hostPort is not disabled. discovery : params : # Corresponds to go-discover provider name provider : \"mdns\" # All other key/values are parameters corresponding to what # the go-discover provider is expecting service : \"rancher-server\" # If this is a new cluster it will wait until 3 server are # available and they all agree on the same cluster-init node expectedServers : 3 # How long servers are remembered for. It is useful for providers # that are not consistent in their responses, like mdns. serverCacheDuration : 1m # The role of this node. Every cluster must start with one node as role=cluster-init. # After that nodes can be joined using the server role for control-plane nodes and # agent role for worker only nodes. The server/agent terms correspond to the server/agent # terms in k3s and RKE2 role : cluster-init,server,agent # The Kubernetes node name that will be set nodeName : custom-hostname # The IP address that will be set in Kubernetes for this node address : 123.123.123.123 # The internal IP address that will be used for this node internalAddress : 123.123.123.124 # Taints to apply to this node upon creation taints : - dedicated=special-user:NoSchedule # Labels to apply to this node upon creation labels : - key=value","title":"Configuration Reference"},{"location":"configuration/#configuration-reference","text":"All configuration should come from RancherOS minimal cloud-init . Below is a reference of supported configuration. It is important that the config always starts with #cloud-config #cloud-config # Add additional users or set the password/ssh keys for root users : - name : \"bar\" passwd : \"foo\" groups : \"users\" ssh_authorized_keys : - faaapploo # Assigns these keys to the first user in users or root if there # is none ssh_authorized_keys : - asdd # Run these commands once the system has fully booted runcmd : - foo # Hostname to assign hostname : \"bar\" # Write arbitrary files write_files : - encoding : b64 content : CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4 path : /foo/bar permissions : \"0644\" owner : \"bar\" # Rancherd configuration rancherd : ######################################################## # The below parameters apply to server role that first # # initializes the cluster # ######################################################## # The Kubernetes version to be installed. This must be a k3s or RKE2 version # v1.21 or newer. k3s and RKE2 versions always have a `k3s` or `rke2` in the # version string. # Valid versions are # k3s: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.k3s.releases[].version' # RKE2: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.rke2.releases[].version' kubernetesVersion : v1.22.2+k3s1 # The Rancher version to be installed or a channel \"latest\" or \"stable\" rancherVersion : v2.6.0 # Values set on the Rancher Helm chart. Refer to # https://github.com/rancher/rancher/blob/release/v2.6/chart/values.yaml # for possible values. rancherValues : # Below are the default values set # Multi-Cluster Management is disabled by default, change to multi-cluster-management=true to enable features : multi-cluster-management=false # The Rancher UI will run on the host port 8443 by default. Set to 0 to disable # and instead use ingress.enabled=true to route traffic through ingress hostPort : 8443 # Accessing ingress is disabled by default. ingress : enabled : false # Don't create a default admin password noDefaultAdmin : true # The negative value means it will up to that many replicas if there are # at least that many nodes available. For example, if you have 2 nodes and # `replicas` is `-3` then 2 replicas will run. Once you add a third node # a then 3 replicas will run replicas : -3 # External TLS is assumed tls : external # Addition SANs (hostnames) to be added to the generated TLS certificate that # served on port 6443. tlsSans : - additionalhostname.example.com # Kubernetes resources that will be created once Rancher is bootstrapped resources : - kind : ConfigMap apiVersion : v1 metadata : name : random data : key : value # Contents of the registries.yaml that will be used by k3s/RKE2. The structure # is documented at https://rancher.com/docs/k3s/latest/en/installation/private-registry/ registries : {} # The default registry used for all Rancher container images. For more information # refer to https://rancher.com/docs/rancher/v2.6/en/admin-settings/config-private-registry/ systemDefaultRegistry : someprefix.example.com:5000 # Advanced: The system agent installer image used for Kubernetes runtimeInstallerImage : ... # Advanced: The system agent installer image used for Rancher rancherInstallerImage : ... # Generic commands to run before bootstrapping the node. preInstructions : - name : something # This image will be extracted to a temporary folder and # set as the current working dir. The command will not run # contained or chrooted, this is only a way to copy assets # to the host. This is parameter is optional image : custom/image:1.1.1 # Environment variables to set env : - FOO=BAR # Program arguments args : - arg1 - arg2 # Command to run command : /bin/dosomething # Save output to /var/lib/rancher/rancherd/plan/plan-output.json saveOutput : false # Generic commands to run after bootstrapping the node. postInstructions : - name : something env : - FOO=BAR args : - arg1 - arg2 command : /bin/dosomething saveOutput : false ########################################### # The below parameters apply to all roles # ########################################### # The URL to Rancher to join a node. If you have disabled the hostPort and configured # TLS then this will be the server you have setup. server : https://myserver.example.com:8443 # A shared secret to join nodes to the cluster token : sometoken # Instead of setting the server parameter above the server value can be dynamically # determined from cloud provider metadata. This is powered by https://github.com/hashicorp/go-discover. # Discovery requires that the hostPort is not disabled. discovery : params : # Corresponds to go-discover provider name provider : \"mdns\" # All other key/values are parameters corresponding to what # the go-discover provider is expecting service : \"rancher-server\" # If this is a new cluster it will wait until 3 server are # available and they all agree on the same cluster-init node expectedServers : 3 # How long servers are remembered for. It is useful for providers # that are not consistent in their responses, like mdns. serverCacheDuration : 1m # The role of this node. Every cluster must start with one node as role=cluster-init. # After that nodes can be joined using the server role for control-plane nodes and # agent role for worker only nodes. The server/agent terms correspond to the server/agent # terms in k3s and RKE2 role : cluster-init,server,agent # The Kubernetes node name that will be set nodeName : custom-hostname # The IP address that will be set in Kubernetes for this node address : 123.123.123.123 # The internal IP address that will be used for this node internalAddress : 123.123.123.124 # Taints to apply to this node upon creation taints : - dedicated=special-user:NoSchedule # Labels to apply to this node upon creation labels : - key=value","title":"Configuration Reference"},{"location":"customizing/","text":"Custom Images \u00b6 RancherOS image can easily be remastered using a docker build. For example, to add cowsay to RancherOS you would use the following Dockerfile Docker image \u00b6 # The version of RancherOS to modify FROM rancher/os2:v0.0.1-test01 # Your custom commands RUN zypper install -y cowsay # IMPORTANT: /usr/lib/rancheros-release is used for versioning/upgrade. The # values here should reflect the tag of the image currently being built ARG IMAGE_REPO = norepo ARG IMAGE_TAG = latest RUN echo \"IMAGE_REPO= ${ IMAGE_REPO } \" > /usr/lib/rancheros-release && \\ echo \"IMAGE_TAG= ${ IMAGE_TAG } \" >> /usr/lib/rancheros-release && \\ echo \"IMAGE= ${ IMAGE_REPO } : ${ IMAGE_TAG } \" >> /usr/lib/rancheros-release And then the following commands docker build --build-arg IMAGE_REPO = myrepo/custom-build \\ --build-arg IMAGE_TAG = v1.1.1 \\ -t myrepo/custom-build:v1.1.1 . docker push myrepo/custom-build:v1.1.1 Your new customized OS is available at in the docker image myrepo/custom-build:v1.1.1 and you can check out your new image using docker with docker run -it myrepo/custom-build:v1.1.1 bash Bootable images \u00b6 To create bootable images from the docker image you just created run the below command # Download the ros-image-build script curl -o ros-image-build https://raw.githubusercontent.com/rancher/os2/main/ros-image-build # Run the script creating a qcow image, an ISO, and an AMI bash ros-image-build myrepo/custom-build:v1.1.1 qcow,iso,ami The above command will create an ISO, a qcow image, and publish AMIs. You need not create all three types and can change to comma seperated list to the types you care for. Auto-installing ISO \u00b6 To create an ISO that upon boot will automatically run an installation, as an alternative to iPXE install, run the following command. bash ros-image-build myrepo/custom-build:v1.1.1 iso mycloud-config-file.txt The third parameter is a path to a file that will be used as the cloud config passed to the installation. Refer to the installation and configuration reference for the contents of the file.","title":"Custom Images"},{"location":"customizing/#custom-images","text":"RancherOS image can easily be remastered using a docker build. For example, to add cowsay to RancherOS you would use the following Dockerfile","title":"Custom Images"},{"location":"customizing/#docker-image","text":"# The version of RancherOS to modify FROM rancher/os2:v0.0.1-test01 # Your custom commands RUN zypper install -y cowsay # IMPORTANT: /usr/lib/rancheros-release is used for versioning/upgrade. The # values here should reflect the tag of the image currently being built ARG IMAGE_REPO = norepo ARG IMAGE_TAG = latest RUN echo \"IMAGE_REPO= ${ IMAGE_REPO } \" > /usr/lib/rancheros-release && \\ echo \"IMAGE_TAG= ${ IMAGE_TAG } \" >> /usr/lib/rancheros-release && \\ echo \"IMAGE= ${ IMAGE_REPO } : ${ IMAGE_TAG } \" >> /usr/lib/rancheros-release And then the following commands docker build --build-arg IMAGE_REPO = myrepo/custom-build \\ --build-arg IMAGE_TAG = v1.1.1 \\ -t myrepo/custom-build:v1.1.1 . docker push myrepo/custom-build:v1.1.1 Your new customized OS is available at in the docker image myrepo/custom-build:v1.1.1 and you can check out your new image using docker with docker run -it myrepo/custom-build:v1.1.1 bash","title":"Docker image"},{"location":"customizing/#bootable-images","text":"To create bootable images from the docker image you just created run the below command # Download the ros-image-build script curl -o ros-image-build https://raw.githubusercontent.com/rancher/os2/main/ros-image-build # Run the script creating a qcow image, an ISO, and an AMI bash ros-image-build myrepo/custom-build:v1.1.1 qcow,iso,ami The above command will create an ISO, a qcow image, and publish AMIs. You need not create all three types and can change to comma seperated list to the types you care for.","title":"Bootable images"},{"location":"customizing/#auto-installing-iso","text":"To create an ISO that upon boot will automatically run an installation, as an alternative to iPXE install, run the following command. bash ros-image-build myrepo/custom-build:v1.1.1 iso mycloud-config-file.txt The third parameter is a path to a file that will be used as the cloud config passed to the installation. Refer to the installation and configuration reference for the contents of the file.","title":"Auto-installing ISO"},{"location":"dashboard/","text":"Dashboard/UI \u00b6 The Rancher UI is running by default on port :8443 . There default admin user password set is a long random striung. You can run rancherd reset-admin to get a new admin password to login. To disable the Rancher UI from running on a host port, or to change the default hostPort used the below configuration. #cloud-config rancherd : rancherValues : # Setting the host port to 0 will disable the hostPort, default is 8443 hostPort : 0","title":"Dashboard/UI"},{"location":"dashboard/#dashboardui","text":"The Rancher UI is running by default on port :8443 . There default admin user password set is a long random striung. You can run rancherd reset-admin to get a new admin password to login. To disable the Rancher UI from running on a host port, or to change the default hostPort used the below configuration. #cloud-config rancherd : rancherValues : # Setting the host port to 0 will disable the hostPort, default is 8443 hostPort : 0","title":"Dashboard/UI"},{"location":"installation/","text":"Installation \u00b6 Overview \u00b6 The design of RancherOS is that you boot from a vanilla image and through cloud-init and Kubernetes mechanisms the node will be configured. Installation of RancherOS is really the process of building an image from which you can boot. During the image building process you can bake in default OEM configuration that is a part of the image. Installation Configuration \u00b6 The installation process is driven by a single config file. The configuration file contains the installation directives and the OEM configuration for the image. The installation configuration should be hosted on an HTTP or TFTP server. A simple approach is to use a GitHub Gist . Kernel Command Line \u00b6 Install directives can be set from the kernel command line using a period (.) seperated key structure such as rancheros.install.configurl . They kernel command line keys are case-insensitive. Reference \u00b6 #cloud-config rancheros : install : # An http://, https://, or tftp:// URL to load as the base configuration # for this configuration. This configuration can include any install # directives or OEM configuration. The resulting merged configuration # will be read by the installer and all content of the merged config will # be stored in /oem/99_custom.yaml in the created image. configURL : http://example.com/machine-cloud-config # Turn on verbose logging for the installation process debug : false # The target device that will be formatted and grub will be install on. # The partition table will be cleared and recreated with the default # partition layout. If noFormat is set to true this parameter is only # used to install grub. device : /dev/vda # If the system has the path /sys/firmware/efi it will be treated as a # UEFI system. If you are creating an UEFI image on a non-EFI platform # then this flag will force the installer to use UEFI even if not detected. forceEFI : false # If true then it is assumed that the disk is already formatted with the standard # partitions need by RancherOS. Refer to the partition table section below for the # exact requirements. Also, if this is set to true noFormat : false # After installation the system will reboot by default. If you wish to instead # power off the system set this to true. powerOff : false # The installed image will set the default console to the current TTY value # used during the installation. To force the installation to use a different TTY # then set that value here. tty : ttyS0 # Any other cloud-init values can be included in this file and will be stored in # /oem/99_custom.yaml of the installed image ISO Installation \u00b6 When booting from the ISO you will immediately be presented with the shell. The root password is hard coded to ros if needed. A SSH server will be running so realize that because of the hard coded password this is an insecure system to be running on a public network. From the shell run the below where ${LOCATION} should be a path to a local file or http:// , https:// , or tftp:// URL. ros-installer -config-file ${ LOCATION } Interactive \u00b6 ros-installer can also be ran without any arguments to allow you to install a simple vanilla image with a root password set. iPXE Installation \u00b6 Download the latest ipxe script from current release Partition Table \u00b6 RancherOS requires the following partitions. These partitions are required by cOS-toolkit Label Default Size Contains COS_BOOT 50 MiB UEFI Boot partition COS_STATE 15 GiB A/B bootable file system images constructed from OCI images COS_OEM 50 MiB OEM cloud-config files and other data COS_RECOVERY 8 GiB Recovery file system image if COS_STATE is destroyed COS_PERSISTENT Remaining space All contents of the persistent folders Folders \u00b6 Path Read-Only Ephemeral Persistent / x /etc x /etc/cni x /etc/iscsi x /etc/rancher x /etc/ssh x /etc/systemd x /srv x /home x /opt x /root x /var x /usr/libexec x /var/lib/cni x /var/lib/kubelet x /var/lib/longhorn x /var/lib/rancher x /var/lib/wicked x /var/log x","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#overview","text":"The design of RancherOS is that you boot from a vanilla image and through cloud-init and Kubernetes mechanisms the node will be configured. Installation of RancherOS is really the process of building an image from which you can boot. During the image building process you can bake in default OEM configuration that is a part of the image.","title":"Overview"},{"location":"installation/#installation-configuration","text":"The installation process is driven by a single config file. The configuration file contains the installation directives and the OEM configuration for the image. The installation configuration should be hosted on an HTTP or TFTP server. A simple approach is to use a GitHub Gist .","title":"Installation Configuration"},{"location":"installation/#kernel-command-line","text":"Install directives can be set from the kernel command line using a period (.) seperated key structure such as rancheros.install.configurl . They kernel command line keys are case-insensitive.","title":"Kernel Command Line"},{"location":"installation/#reference","text":"#cloud-config rancheros : install : # An http://, https://, or tftp:// URL to load as the base configuration # for this configuration. This configuration can include any install # directives or OEM configuration. The resulting merged configuration # will be read by the installer and all content of the merged config will # be stored in /oem/99_custom.yaml in the created image. configURL : http://example.com/machine-cloud-config # Turn on verbose logging for the installation process debug : false # The target device that will be formatted and grub will be install on. # The partition table will be cleared and recreated with the default # partition layout. If noFormat is set to true this parameter is only # used to install grub. device : /dev/vda # If the system has the path /sys/firmware/efi it will be treated as a # UEFI system. If you are creating an UEFI image on a non-EFI platform # then this flag will force the installer to use UEFI even if not detected. forceEFI : false # If true then it is assumed that the disk is already formatted with the standard # partitions need by RancherOS. Refer to the partition table section below for the # exact requirements. Also, if this is set to true noFormat : false # After installation the system will reboot by default. If you wish to instead # power off the system set this to true. powerOff : false # The installed image will set the default console to the current TTY value # used during the installation. To force the installation to use a different TTY # then set that value here. tty : ttyS0 # Any other cloud-init values can be included in this file and will be stored in # /oem/99_custom.yaml of the installed image","title":"Reference"},{"location":"installation/#iso-installation","text":"When booting from the ISO you will immediately be presented with the shell. The root password is hard coded to ros if needed. A SSH server will be running so realize that because of the hard coded password this is an insecure system to be running on a public network. From the shell run the below where ${LOCATION} should be a path to a local file or http:// , https:// , or tftp:// URL. ros-installer -config-file ${ LOCATION }","title":"ISO Installation"},{"location":"installation/#interactive","text":"ros-installer can also be ran without any arguments to allow you to install a simple vanilla image with a root password set.","title":"Interactive"},{"location":"installation/#ipxe-installation","text":"Download the latest ipxe script from current release","title":"iPXE Installation"},{"location":"installation/#partition-table","text":"RancherOS requires the following partitions. These partitions are required by cOS-toolkit Label Default Size Contains COS_BOOT 50 MiB UEFI Boot partition COS_STATE 15 GiB A/B bootable file system images constructed from OCI images COS_OEM 50 MiB OEM cloud-config files and other data COS_RECOVERY 8 GiB Recovery file system image if COS_STATE is destroyed COS_PERSISTENT Remaining space All contents of the persistent folders","title":"Partition Table"},{"location":"installation/#folders","text":"Path Read-Only Ephemeral Persistent / x /etc x /etc/cni x /etc/iscsi x /etc/rancher x /etc/ssh x /etc/systemd x /srv x /home x /opt x /root x /var x /usr/libexec x /var/lib/cni x /var/lib/kubelet x /var/lib/longhorn x /var/lib/rancher x /var/lib/wicked x /var/log x","title":"Folders"},{"location":"mcm/","text":"Multi-Cluster Management \u00b6 By default Multi Cluster Managmement is disabled in Rancher. To enable set the following in the rancherd config.yaml #cloud-config rancherd : rancherValues : features : - multi-cluster-management=true","title":"Multi-Cluster Management"},{"location":"mcm/#multi-cluster-management","text":"By default Multi Cluster Managmement is disabled in Rancher. To enable set the following in the rancherd config.yaml #cloud-config rancherd : rancherValues : features : - multi-cluster-management=true","title":"Multi-Cluster Management"},{"location":"operator/","text":"Operator \u00b6 The RancherOS operator is responsible for managing the RancherOS versions and maintaining a machine inventory to assist with edge or baremetal installations. Installation \u00b6 The RancherOS operator can be added to a cluster running Rancher Multi Cluster Management server. It is a helm chart and can be installed as follows: helm -n cattle-rancheros-operator-system install --create-namespace rancheros-operator https://github.com/rancher/os2/releases/download/v0.1.0-alpha12/rancheros-operator-0.1.0-alpha12-amd64.tgz Managing Upgrades \u00b6 The RancherOS operator will manage the upgrade of the local cluster where the operator is running and also any downstream cluster managed by Rancher Multi-Cluster Manager. ManagedOSImage \u00b6 The ManagedOSImage kind used to define what version of RancherOS should be running on each node. The simplest example of this type would be to change the version of the local nodes. kubectl edit -n fleet-local default-os-image apiVersion : rancheros.cattle.io/v1 kind : ManagedOSImage metadata : name : default-os-image namespace : fleet-local spec : osImage : rancher/os2:v0.0.0 Reference \u00b6 Below is reference of the full type apiVersion : rancheros.cattle.io/v1 kind : ManagedOSImage metadata : name : arbitrary # There are two special namespaces to consider. If you wish to manage # nodes on the local cluster this namespace must be `fleet-local`. If # you wish to manage nodes in Rancher MCM managed clusters then the # namespace must match the namespace of the clusters.provisioning.cattle.io resource # which is typically fleet-default. namespace : fleet-local spec : # The image name to pull for the OS osImage : rancher/os2:v0.0.0 # The selector for which nodes will be select. If null then all nodes # will be selected nodeSelector : matchLabels : {} # How many nodes in parallel to update. If empty the default is 1 and # if set to 0 the rollout will be paused concurrency : 2 # Arbitrary action to perform on the node prior to upgrade prepare : image : ubuntu command : [ \"/bin/sh\" ] args : [ \"-c\" , \"true\" ] env : - name : TEST_ENV value : testValue # Parameters to control the drain behavior. If null no draining will happen # on the node. drain : # Refer to kubectl drain --help for the definition of these values timeout : 5m gracePeriod : 5m deleteLocalData : false ignoreDaemonSets : true force : false disableEviction : false skipWaitForDeleteTimeout : 5 # Which clusters to target # This is used if you are running Rancher MCM and managing # multiple clusters. The syntax of this field matches the # Fleet targets and is described at https://fleet.rancher.io/gitrepo-targets/ targets : [] Inventory Management \u00b6 The RancherOS operator can hold an inventory of machines and the mapping of the machine to it's configuration and assigned cluster. MachineInventory \u00b6 Reference \u00b6 apiVersion : rancheros.cattle.io/v1 kind : MachineInventory metadata : name : machine-a # The namespace must match the namespace of the cluster # assigned to the clusters.provisioning.cattle.io resource namespace : fleet-default spec : # The cluster that this machine is assigned to clusterName : some-cluster # The hash of the TPM EK public key. This is used if you are # using TPM2 to identifiy nodes. You can obtain the TPM by # running `rancherd get-tpm-hash` on the node. Or nodes can # report their TPM hash by using the MachineRegister tpm : d68795c6192af9922692f050b... # Generic SMBIOS fields that are typically populated with # the MachineRegister approach smbios : {} # A reference to a secret that contains a shared secret value to # identify a node. The secret must be of type \"rancheros.cattle.io/token\" # and have on field \"token\" which is the value of the shared secret machineTokenSecretName : some-secret-name # Arbitrary cloud config that will be added to the machines cloud config # during the rancherd bootstrap phase. The one important field that should # be set is the role. config : role : server MachineRegistration \u00b6 Reference \u00b6 kind : MachineRegistration metadata : name : machine-registration # The namespace must match the namespace of the cluster # assigned to the clusters.provisioning.cattle.io resource namespace : fleet-default spec : # Labels to be added to the created MachineInventory object machineInventoryLabels : {} # Annotations to be added to the created MachineInventory object machineInventoryAnnotations : {} # The cloud config that will be used to provision the node cloudConfig : {}","title":"Operator"},{"location":"operator/#operator","text":"The RancherOS operator is responsible for managing the RancherOS versions and maintaining a machine inventory to assist with edge or baremetal installations.","title":"Operator"},{"location":"operator/#installation","text":"The RancherOS operator can be added to a cluster running Rancher Multi Cluster Management server. It is a helm chart and can be installed as follows: helm -n cattle-rancheros-operator-system install --create-namespace rancheros-operator https://github.com/rancher/os2/releases/download/v0.1.0-alpha12/rancheros-operator-0.1.0-alpha12-amd64.tgz","title":"Installation"},{"location":"operator/#managing-upgrades","text":"The RancherOS operator will manage the upgrade of the local cluster where the operator is running and also any downstream cluster managed by Rancher Multi-Cluster Manager.","title":"Managing Upgrades"},{"location":"operator/#managedosimage","text":"The ManagedOSImage kind used to define what version of RancherOS should be running on each node. The simplest example of this type would be to change the version of the local nodes. kubectl edit -n fleet-local default-os-image apiVersion : rancheros.cattle.io/v1 kind : ManagedOSImage metadata : name : default-os-image namespace : fleet-local spec : osImage : rancher/os2:v0.0.0","title":"ManagedOSImage"},{"location":"operator/#reference","text":"Below is reference of the full type apiVersion : rancheros.cattle.io/v1 kind : ManagedOSImage metadata : name : arbitrary # There are two special namespaces to consider. If you wish to manage # nodes on the local cluster this namespace must be `fleet-local`. If # you wish to manage nodes in Rancher MCM managed clusters then the # namespace must match the namespace of the clusters.provisioning.cattle.io resource # which is typically fleet-default. namespace : fleet-local spec : # The image name to pull for the OS osImage : rancher/os2:v0.0.0 # The selector for which nodes will be select. If null then all nodes # will be selected nodeSelector : matchLabels : {} # How many nodes in parallel to update. If empty the default is 1 and # if set to 0 the rollout will be paused concurrency : 2 # Arbitrary action to perform on the node prior to upgrade prepare : image : ubuntu command : [ \"/bin/sh\" ] args : [ \"-c\" , \"true\" ] env : - name : TEST_ENV value : testValue # Parameters to control the drain behavior. If null no draining will happen # on the node. drain : # Refer to kubectl drain --help for the definition of these values timeout : 5m gracePeriod : 5m deleteLocalData : false ignoreDaemonSets : true force : false disableEviction : false skipWaitForDeleteTimeout : 5 # Which clusters to target # This is used if you are running Rancher MCM and managing # multiple clusters. The syntax of this field matches the # Fleet targets and is described at https://fleet.rancher.io/gitrepo-targets/ targets : []","title":"Reference"},{"location":"operator/#inventory-management","text":"The RancherOS operator can hold an inventory of machines and the mapping of the machine to it's configuration and assigned cluster.","title":"Inventory Management"},{"location":"operator/#machineinventory","text":"","title":"MachineInventory"},{"location":"operator/#reference_1","text":"apiVersion : rancheros.cattle.io/v1 kind : MachineInventory metadata : name : machine-a # The namespace must match the namespace of the cluster # assigned to the clusters.provisioning.cattle.io resource namespace : fleet-default spec : # The cluster that this machine is assigned to clusterName : some-cluster # The hash of the TPM EK public key. This is used if you are # using TPM2 to identifiy nodes. You can obtain the TPM by # running `rancherd get-tpm-hash` on the node. Or nodes can # report their TPM hash by using the MachineRegister tpm : d68795c6192af9922692f050b... # Generic SMBIOS fields that are typically populated with # the MachineRegister approach smbios : {} # A reference to a secret that contains a shared secret value to # identify a node. The secret must be of type \"rancheros.cattle.io/token\" # and have on field \"token\" which is the value of the shared secret machineTokenSecretName : some-secret-name # Arbitrary cloud config that will be added to the machines cloud config # during the rancherd bootstrap phase. The one important field that should # be set is the role. config : role : server","title":"Reference"},{"location":"operator/#machineregistration","text":"","title":"MachineRegistration"},{"location":"operator/#reference_2","text":"kind : MachineRegistration metadata : name : machine-registration # The namespace must match the namespace of the cluster # assigned to the clusters.provisioning.cattle.io resource namespace : fleet-default spec : # Labels to be added to the created MachineInventory object machineInventoryLabels : {} # Annotations to be added to the created MachineInventory object machineInventoryAnnotations : {} # The cloud config that will be used to provision the node cloudConfig : {}","title":"Reference"},{"location":"upgrade/","text":"Upgrade \u00b6 Command line \u00b6 You can also use the rancherd upgrade command on a server node to automatically upgrade RancherOS, Rancher, and/or Kubernetes. Kubernetes API \u00b6 All components in RancherOS are managed using Kubernetes. Below is how to use Kubernetes approaches to upgrade the components. RancherOS \u00b6 RancherOS is upgraded with the RancherOS operator. Refer to the RancherOS Operator documentation for complete information, but the TL;DR is kubectl edit -n fleet-local default-os-image apiVersion : rancheros.cattle.io/v1 kind : ManagedOSImage metadata : name : default-os-image namespace : fleet-local spec : # Set to the new RancherOS version you would like to upgrade to osImage : rancher/os2:v0.0.0 rancherd \u00b6 Rancherd itself doesn't need to be upgraded. It is only ran once per node to bootstrap the system and then after that provides no value. Rancherd is packaged in the OS image so newer versions of Rancherd will come with newer versions of RancherOS. Rancher \u00b6 Rancher is installed as a helm chart following the standard procedure. You can upgrade Rancher with the standard procedure documented . Kubernetes \u00b6 To upgrade Kubernetes you will use Rancher to orchestrate the upgrade. This is a matter of changing the Kubernetes version on the fleet-local/local Cluster in the provisioning.cattle.io/v1 apiVersion. For example kubectl edit clusters.provisioning.cattle.io -n fleet-local local apiVersion : provisioning.cattle.io/v1 kind : Cluster metadata : name : local namespace : fleet-local spec : # Change to new valid k8s version, >= 1.21 # Valid versions are # k3s: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.k3s.releases[].version' # RKE2: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.rke2.releases[].version' kubernetesVersion : v1.21.4+k3s1","title":"Upgrade"},{"location":"upgrade/#upgrade","text":"","title":"Upgrade"},{"location":"upgrade/#command-line","text":"You can also use the rancherd upgrade command on a server node to automatically upgrade RancherOS, Rancher, and/or Kubernetes.","title":"Command line"},{"location":"upgrade/#kubernetes-api","text":"All components in RancherOS are managed using Kubernetes. Below is how to use Kubernetes approaches to upgrade the components.","title":"Kubernetes API"},{"location":"upgrade/#rancheros","text":"RancherOS is upgraded with the RancherOS operator. Refer to the RancherOS Operator documentation for complete information, but the TL;DR is kubectl edit -n fleet-local default-os-image apiVersion : rancheros.cattle.io/v1 kind : ManagedOSImage metadata : name : default-os-image namespace : fleet-local spec : # Set to the new RancherOS version you would like to upgrade to osImage : rancher/os2:v0.0.0","title":"RancherOS"},{"location":"upgrade/#rancherd","text":"Rancherd itself doesn't need to be upgraded. It is only ran once per node to bootstrap the system and then after that provides no value. Rancherd is packaged in the OS image so newer versions of Rancherd will come with newer versions of RancherOS.","title":"rancherd"},{"location":"upgrade/#rancher","text":"Rancher is installed as a helm chart following the standard procedure. You can upgrade Rancher with the standard procedure documented .","title":"Rancher"},{"location":"upgrade/#kubernetes","text":"To upgrade Kubernetes you will use Rancher to orchestrate the upgrade. This is a matter of changing the Kubernetes version on the fleet-local/local Cluster in the provisioning.cattle.io/v1 apiVersion. For example kubectl edit clusters.provisioning.cattle.io -n fleet-local local apiVersion : provisioning.cattle.io/v1 kind : Cluster metadata : name : local namespace : fleet-local spec : # Change to new valid k8s version, >= 1.21 # Valid versions are # k3s: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.k3s.releases[].version' # RKE2: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.rke2.releases[].version' kubernetesVersion : v1.21.4+k3s1","title":"Kubernetes"},{"location":"versions/","text":"Supported Versions and Channels \u00b6 The kubernetesVersion and rancherVersion fields accept explicit versions numbers or channel names. Valid Versions \u00b6 The list of valid versions for the kubernetesVersion field can be determined from the Rancher metadata using the following commands. k3s: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.k3s.releases[].version' rke2: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.rke2.releases[].version' The list of valid rancherVersion values can be obtained from the stable and latest helm repos. The version string is expected to be the \"application version\" which is the version starting with a v . For example, v2.6.2 is the current format not 2.6.2 . Version Channels \u00b6 Valid kubernetesVersion channels are as follows: Channel Name Description stable k3s stable (default value of kubernetesVersion) latest k3s latest testing k3s test stable:k3s Same as stable channel latest:k3s Same as latest channel testing:k3s Same as testing channel stable:rke2 rke2 stable latest:rke2 rke2 latest testing:rke2 rke2 testing v1.21 Latest k3s v1.21 release. The applies to any Kubernetes minor version v1.21:rke2 Latest rke2 v1.21 release. The applies to any Kubernetes minor version Valid rancherVersions channels are as follows: Channel Name Description stable stable helm repo latest latest helm repo","title":"Supported Versions and Channels"},{"location":"versions/#supported-versions-and-channels","text":"The kubernetesVersion and rancherVersion fields accept explicit versions numbers or channel names.","title":"Supported Versions and Channels"},{"location":"versions/#valid-versions","text":"The list of valid versions for the kubernetesVersion field can be determined from the Rancher metadata using the following commands. k3s: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.k3s.releases[].version' rke2: curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.rke2.releases[].version' The list of valid rancherVersion values can be obtained from the stable and latest helm repos. The version string is expected to be the \"application version\" which is the version starting with a v . For example, v2.6.2 is the current format not 2.6.2 .","title":"Valid Versions"},{"location":"versions/#version-channels","text":"Valid kubernetesVersion channels are as follows: Channel Name Description stable k3s stable (default value of kubernetesVersion) latest k3s latest testing k3s test stable:k3s Same as stable channel latest:k3s Same as latest channel testing:k3s Same as testing channel stable:rke2 rke2 stable latest:rke2 rke2 latest testing:rke2 rke2 testing v1.21 Latest k3s v1.21 release. The applies to any Kubernetes minor version v1.21:rke2 Latest rke2 v1.21 release. The applies to any Kubernetes minor version Valid rancherVersions channels are as follows: Channel Name Description stable stable helm repo latest latest helm repo","title":"Version Channels"}]}